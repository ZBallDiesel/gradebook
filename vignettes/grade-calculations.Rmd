---
title: "Grade Calculations"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Nested Grouped Operations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r}
#| message: false
library(tidyverse)
policy <- list(list(category = "ps_1",
                    aggregation = "weighted_by_points",
                    assignments = c("ps_1_written", "ps_1_code")),
               list(category = "ps_2",
                    aggregation = "weighted_by_points",
                    assignments = c("ps_2_written", "ps_2_code")),
               list(category = "problem_sets",
                    aggregation = "equally_weighted",
                    assignments = c("ps_1", "ps_2")),
               list(category = "exam",
                    aggregation = "none",
                    assignments = c("final_exam")))

grades_b <- tibble(student_id = rep(c(4863, 5211, 4893), 
                                    each = 5),
                   assignment = rep(c("ps_1_written", 
                                      "ps_1_code",
                                      "ps_2_written",
                                      "ps_2_code",
                                      "final_exam"), 3),
                   pts_earned = c(12, 5, 19, 13, 86,
                                  11, 6, 19, 10, 75,
                                  8, 8, 10, 13, 92),
                   pts_max    = rep(c(15, 10, 20, 16, 100), 3),
                   category   = rep(c("ps", "ps", "ps", "ps", "exam"), 3),
                   subcategory = rep(c("ps_1", "ps_1", "ps_2", "ps_2", "exam"), 3))

gs_wide <- grades_b |>
    mutate(score = pts_earned / pts_max) |>
    pivot_wider(id_cols = student_id,
                names_from = assignment,
                values_from = score) |>
    mutate(ps_1_written_max_pts = 15,
           ps_1_code_max_pts = 10,
           ps_2_written_max_pts = 20,
           ps_2_code_max_pts = 16,
           final_exam_max_pts = 100)
gs_wide
```

## Aggregation Functions

#### Vector Versions

```{r}
#| eval: true
equally_weighted <- function(assignments, weights, n_drops, ...) {

    if (n_drops > 0) { assignments[order(assignments)[1:n_drops]] <- NA}

    mean(assignments, na.rm =TRUE)
}

weighted_by_points <- function(assignments, weights, n_drops) {

    if (n_drops > 0) {
        drop_idx <- order(assignments)[1:n_drops]
        weights[drop_idx] <- NA
        assignments[drop_idx] <- NA
    }

    sum(assignments * (weights / sum(weights, na.rm = TRUE)), na.rm =TRUE)
}

max_score <- function(assignments, ...) {
    max(assignments)
}

min_score <- function(assignments, ...) {
    min(assignments)
}

none <- function(assignments, ...) {
    assignments
}

```

## Create aggregation functions

```{r}
agg_assigns <- function(gs_wide, policy_item) {
    get(policy_item$aggregation)(
        assignments = gs_wide[policy_item$assignments],
        weights = gs_wide[paste0(policy_item$assignments, "_max_pts")],
        n_drops = ifelse(is.null(policy_item$n_drops), 0, policy_item$n_drops))
}

apply_policy <- function(policy_item, gs_wide) {
    gs_wide[[policy_item$category]] <<- apply(gs_wide, 1, agg_assigns, policy_item = policy_item)
}
```

## Run computations

### Version A: Walk and apply

```{r}
gs_wide_forloop <- gs_wide
library(microbenchmark)
microbenchmark({
    purrr::walk(policy, \(x) apply_policy(policy_item = x, gs_wide = gs_wide))
})
```

Places for improvement:

1. Inner `apply()` can possibly be parrallelize. Possibly by switching to `map()` family and using `furrr`.
2. Global assignment operator, `<<-` is very sketchy to use. It be good if we can find an alternative.

### Version B: Naive double for loop

This version doesn't currently calculate the `problem_set` category correctly, but it is much slower.

```{r}
microbenchmark({
    for(i in 1:length(policy)) { 
        for(j in 1:nrow(gs_wide_forloop)) {
            gs_wide_forloop[j, policy[[i]]$category] <- agg_assigns(policy_item = policy[[i]],
                                                              gs_wide_forloop[j,])
        }
    }
})
```




## Appendix

### Comparison of drop then reweight and reweight then drop

Looks like drop then reweight is the safer bet - usually gives the higher score (and easier to think about).


```{r}
assignments <- c(.8, .5, .9)
weights <- c(1, 3, .2)
n_drops <- 2

normalize_weights <- function(weights) {
    weights / sum(weights, na.rm = TRUE)
}

# drop first
if (n_drops > 0) {
    drop_idx <- order(assignments)[1:n_drops]
    weights[drop_idx] <- NA
    assignments[drop_idx] <- NA
}

weights <- normalize_weights(weights)
sum(assignments * weights, na.rm =TRUE)

assignments <- c(.8, .5, .9)
weights <- c(1, 3, .2)

weights <- normalize_weights(weights)
weighted_assignments <- assignments * weights

# drop second
if (n_drops > 0) {
    drop_idx <- order(weighted_assignments)[1:n_drops]
    weights[drop_idx] <- NA
    weighted_assignments[drop_idx] <- NA
}

sum(weighted_assignments / sum(weights, na.rm = TRUE), na.rm = TRUE)
```





